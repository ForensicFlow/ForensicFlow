# SpectraX AI Agent Framework Documentation

## Overview

SpectraX has been upgraded from a simple Q&A chatbot to a sophisticated AI agent with:
1. **Enhanced conversation memory** - Tracks context across 10+ exchanges
2. **Tool-use capabilities** - Can generate graphs, timelines, and perform analyses
3. **Reliable structured outputs** - Uses Gemini Function Calling API

## Architecture

### 1. Conversation Manager (`ConversationManager` class)

**Location:** `backend/ai_analysis/ai_service.py` (lines 15-137)

**Purpose:** Maintains conversation state and context across multiple turns.

**Features:**
- Tracks last 10 exchanges with timestamps
- Extracts and tracks entities (phones, emails, crypto addresses)
- Estimates token usage to manage context window
- Provides context summaries for the LLM

**Usage:**
```python
conv_manager = ConversationManager(max_exchanges=10, max_tokens=8000)
conv_manager.add_exchange(query, response, metadata)
context_summary = conv_manager.get_context_summary()
```

### 2. Tool Registry (`ToolRegistry` class)

**Location:** `backend/ai_analysis/tools.py`

**Purpose:** Provides executable tools that the AI agent can call to perform actions.

**Available Tools:**

1. **`generate_network_graph`**
   - Creates network visualization JSON
   - Parameters: nodes[], edges[]
   - Returns: Network graph data structure

2. **`generate_timeline`**
   - Creates timeline visualization
   - Parameters: events[]
   - Returns: Sorted timeline events

3. **`search_evidence`**
   - Searches evidence with filters
   - Parameters: query, evidence_types, date_range, limit
   - Returns: Filtered evidence items

4. **`analyze_pattern`**
   - Performs pattern analysis
   - Parameters: evidence_ids, analysis_type, focus
   - Returns: Pattern analysis results

5. **`get_entity_details`**
   - Gets information about specific entities
   - Parameters: entity_type, entity_value
   - Returns: Entity details and related evidence

6. **`format_report_section`**
   - Formats content into report sections
   - Parameters: section_type, content, evidence_ids
   - Returns: Formatted report section

**Usage:**
```python
from ai_analysis.tools import ToolRegistry

tool_registry = ToolRegistry(evidence_data=evidence, case_id=case_id)
result = tool_registry.execute_tool('generate_network_graph', {
    'nodes': [...],
    'edges': [...]
})
```

### 3. Function Schemas

**Location:** `backend/ai_analysis/function_schemas.py`

**Purpose:** Defines tool schemas for Gemini Function Calling API.

**Schema Structure:**
```python
{
    "name": "tool_name",
    "description": "What the tool does",
    "parameters": {
        "type": "object",
        "properties": {
            "param1": {"type": "string", "description": "..."},
            "param2": {"type": "array", "items": {...}}
        },
        "required": ["param1"]
    }
}
```

### 4. Agent Processing Loop (ReAct Pattern)

**Location:** `backend/ai_analysis/ai_service.py` (method `process_query_with_agent`)

**Flow:**
1. **Reasoning:** Gemini decides which tools to use
2. **Action:** Execute the tools
3. **Observation:** Get tool results
4. **Response:** Generate final answer incorporating tool results

**Example Flow:**
```
User: "Show me a network graph"
  → Reasoning: Need to generate graph visualization
  → Action: Call generate_network_graph tool
  → Observation: Received graph JSON with 15 nodes, 23 edges
  → Response: "Here is the network graph showing connections..." + JSON data
```

## API Changes

### `/ai/queries/ask/` Endpoint

**New Response Format:**
```json
{
    "query": {...},
    "summary": "AI-generated analysis...",
    "evidence": [...],
    "results_count": 10,
    "confidence": 0.92,
    "processing_time": 2.3,
    "suggested_followups": ["...", "..."],
    "embedded_component": {
        "type": "network|timeline|map|chat_bubbles",
        "data": {...}
    }
}
```

**Key Changes:**
- `embedded_component` is now generated by agent tools
- Response includes agent execution metadata
- Higher confidence scores when tools are used

## Configuration

### Enable/Disable Agent Mode

In `ai_service.py`:
```python
def __init__(self):
    self.enable_agent_mode = True  # Set to False to disable
    self.max_tool_iterations = 5   # Max tool calls per query
```

### Required Environment Variables

```bash
# .env file
GEMINI_API_KEY=your_gemini_api_key_here
```

## Frontend Integration

**Location:** `client/components/SpectraX.tsx`

**Handling Agent Responses:**

The frontend automatically renders embedded components from agent responses:

```typescript
if (response.embedded_component) {
    if (response.embedded_component.type === 'network') {
        // Render NetworkGraph component
    } else if (response.embedded_component.type === 'timeline') {
        // Render MiniTimeline component
    }
}
```

**No changes required** - existing frontend code already supports agent-generated visualizations.

## Testing

### Manual Testing Queries

Test the agent framework with these queries:

1. **Network Graph Generation:**
   - "Show me a network graph"
   - "What are the connections between entities?"
   - "Show me the relationship network"

2. **Timeline Generation:**
   - "Create a timeline of events"
   - "Show me what happened chronologically"
   - "Timeline for March 15th"

3. **Pattern Analysis:**
   - "What patterns do you see?"
   - "Analyze communication frequency"
   - "Find suspicious behavior"

4. **Contextual Queries:**
   - First: "Find crypto transactions"
   - Then: "Show me a graph of those" (tests context memory)
   - Then: "When did those occur?" (tests entity tracking)

### Expected Behavior

✅ **Success Indicators:**
- Agent mode log appears: `[Agent Mode] Processing query: ...`
- Tool execution logs: `[Agent Mode] Tool call: generate_network_graph`
- Response includes: `> **Agent Mode:** Used X tool(s) to generate this response.`
- Visualizations render automatically

❌ **Fallback Indicators:**
- `[Agent Mode] Falling back to regular query processing`
- No tool execution logs
- Standard text-only response

## Performance Optimization

### Caching

Tool results are cached per query to avoid redundant API calls:
```python
cache_key = f"nlp_query_{query_hash}_{evidence_hash}"
cached_result = cache.get(cache_key)
```

### Token Management

The ConversationManager automatically:
- Truncates old exchanges when context is too large
- Limits response length in conversation history
- Estimates token usage to stay under limits

## Error Handling

### Graceful Degradation

If agent mode fails, the system automatically falls back to regular processing:

```python
try:
    # Try agent mode with tools
    return process_query_with_agent(...)
except Exception as e:
    # Fallback to regular mode
    return process_natural_language_query(...)
```

### Common Issues

1. **Tool Execution Failure:**
   - System logs error but continues
   - Returns text-only response
   - No user-facing error

2. **Gemini API Timeout:**
   - 30-second timeout on initial call
   - 20-second timeout on follow-up
   - Auto-fallback to regular processing

3. **Invalid Tool Parameters:**
   - Tool validation catches errors
   - Returns error in tool result
   - Agent incorporates error in response

## Debugging

### Enable Verbose Logging

Check console output for:
```
[Agent Mode] Processing query: ...
[Agent Mode] Suggested tools: ['generate_network_graph']
[Agent Mode] Tool call: generate_network_graph with args: {...}
[Agent Mode] Tool result: success=True
[Response] Including embedded component: network
```

### Check Tool Execution Log

Access via:
```python
tool_registry = ToolRegistry(...)
# ... execute tools ...
print(tool_registry.execution_log)
```

## Future Enhancements

Potential improvements:

1. **Multi-turn tool use** - Allow agent to call tools in sequence
2. **Parallel tool execution** - Run multiple tools simultaneously
3. **Tool result caching** - Cache expensive tool operations
4. **Custom tool definition** - Allow users to define new tools
5. **Agent reflection** - Let agent review and improve its outputs

## Monitoring

### Key Metrics to Track

- Agent activation rate (% queries using tools)
- Tool success rate
- Average tools per query
- Response time with/without tools
- User satisfaction with agent responses

### Logs to Monitor

- `[Agent Mode]` entries in application logs
- Tool execution errors
- Fallback occurrences
- API timeout events

## Support

For issues or questions:
1. Check logs for `[Agent Mode]` entries
2. Verify Gemini API key is configured
3. Test with simple queries first
4. Check tool execution log for errors
5. Review this documentation for common patterns

## Version History

- **v2.0** (Current) - Full agent framework with tool use
- **v1.5** - Enhanced conversation memory
- **v1.0** - Basic Q&A functionality

